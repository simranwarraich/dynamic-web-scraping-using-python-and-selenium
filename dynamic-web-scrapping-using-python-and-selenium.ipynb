{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Web Scraping Beauty Website Using Python and Selenium\nIn the vast landscape of web scraping, there are websites that willingly share their data, and then there are those that guard it behind layers of JavaScript, dynamic content, and fancy designs. The cosmetics industry, known for its alluring products and ever-changing trends, is no exception. In this project, I'll take you on a journey where I scraped the Tira Beauty website, a treasure trove of skincare and makeup products, and reveal the secrets behind it. This project is not just about the beauty of cosmetics but also the beauty of web scraping using Python and Selenium.","metadata":{}},{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!pip install selenium # install selenium for web scraping","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Install the necessary tools and libraries. \nfrom bs4 import BeautifulSoup\nimport requests\nimport pandas as pd\nimport numpy as np\nimport time\nfrom selenium import webdriver\nfrom selenium.webdriver.common.by import By\nimport random\nfrom selenium.webdriver.common.keys import Keys","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# list of User-Agent strings to mimic real user behavior \nuser_agents = [\n    \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/92.0.4515.159 Safari/537.36\",\n    \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36\",\n    \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/90.0.4430.212 Safari/537.36\",\n    \"Mozilla/5.0 (iPhone14,3; U; CPU iPhone OS 15_0 like Mac OS X) AppleWebKit/602.1.50 (KHTML, like Gecko) Version/10.0 Mobile/19A346 Safari/602.1\"\n    \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/109.0.0.0 Safari/537.36\"\n    # Add more User-Agent strings as needed\n]","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Set up Chrome in headless mode\n# The webpage URL\nURL = \"https://www.tirabeauty.com/collection/skin\"\n# Set up the WebDriver (in this case, using Chrome)\nchrome_options = webdriver.ChromeOptions()\nchrome_options.add_argument('--no-sandbox')\nchrome_options.add_argument('--headless')\nchrome_options.add_argument('--disable-gpu')\nchrome_options.add_argument('--disable-dev-shm-usage')\nchrome_options.add_argument(\"--window-size=1920,1080\")\nrandom_user_agent = random.choice(user_agents)\nchrome_options.add_argument(f\"--user-agent={random_user_agent}\")\ndriver = webdriver.Chrome(options=chrome_options)\n# Open the webpage in the browser\ndriver.get(URL)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"To load more products, we scroll down the page multiple times, hitting the END key like a seasoned explorer. We pause for a moment, allowing the content to load and unveil itself, and repeat the process a few times.","metadata":{}},{"cell_type":"code","source":"# Scroll down multiple times to load more products (adjust the number of times as needed)\nfor _ in range(3):  # You can increase the number of iterations for more scrolling\n    driver.find_element(By.TAG_NAME,'body').send_keys(Keys.END)\n    time.sleep(5)  # Wait for the page to load (adjust the time as needed)\n\n# Get the page source after scrolling\npage_source = driver.page_source\n\n# Close the browser\ndriver.quit()\n\n# Soup Object containing all data\nsoup = BeautifulSoup(page_source, \"html.parser\")\n\n# Fetch links as List of Tag Objects\nlinks = soup.find_all(\"a\", attrs={'class': 'product-wrap'})\n\n# Store the links\nlinks_list = []\n\n# Loop for extracting links from Tag Objects\nfor link in links:\n    links_list.append(link.get('href'))\n\nd = {\"title\":[], \"current_price\":[], \"old_price\":[],\"discount\":[],\"rating\":[], \"reviews\":[]}\n    \nlinks_list","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# install google chrome\n!wget https://dl.google.com/linux/linux_signing_key.pub\n!sudo apt-key add linux_signing_key.pub\n!echo 'deb [arch=amd64] http://dl.google.com/linux/chrome/deb/ stable main' >> /etc/apt/sources.list.d/google-chrome.list\n!sudo apt-get -y update\n!sudo apt-get install -y google-chrome-stable\n\n# install chromedriver\n!wget -O /tmp/chromedriver.zip https://edgedl.me.gvt1.com/edgedl/chrome/chrome-for-testing/117.0.5938.92/linux64/chromedriver-linux64.zip\n!unzip /tmp/chromedriver.zip chromedriver-linux64/chromedriver -d /usr/local/bin/\n\n#move the chrome driver executable to correct path\n!mv /usr/local/bin/chromedriver-linux64/chromedriver /usr/local/bin/chromedriver","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# To check Google Chrome's version\n!google-chrome --version\n\n# To check Chrome Driver's version\n!chromedriver -v","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Function to scrape a web page using Selenium\ndef get_price_using_selenium(url):\n    # Initialize ChromeOptions to configure the WebDriver\n    chrome_options = webdriver.ChromeOptions()\n    \n    # Configure ChromeOptions for headless browsing (without a visible browser window)\n    chrome_options.add_argument('--no-sandbox')\n    chrome_options.add_argument('--headless')\n    chrome_options.add_argument('--disable-gpu')\n    chrome_options.add_argument('--disable-dev-shm-usage')\n    chrome_options.add_argument(\"--window-size=1920,1080\")\n    \n    # Randomly select a User-Agent to mimic real browser behavior\n    random_user_agent = random.choice(user_agents)\n    chrome_options.add_argument(f\"--user-agent={random_user_agent}\")\n    \n    # Create a WebDriver instance using ChromeOptions\n    driver = webdriver.Chrome(options=chrome_options)\n    \n    # Navigate to the specified URL\n    driver.get(url)\n    \n    # Wait for some time (e.g., 15 seconds) to allow the JavaScript content to load\n    time.sleep(15)\n    \n    # Extract data from the web page using helper functions\n    d['title'].append(get_element_using_selenium(driver, 'ID', 'item_name'))\n    d['current_price'].append(get_element_using_selenium(driver, 'ID', 'item_price'))\n    d['old_price'].append(get_element_using_selenium(driver, 'CLASS_NAME', 'old-amount'))\n    d['discount'].append(get_element_using_selenium(driver, 'CLASS_NAME', 'save-amount-per'))\n    d['rating'].append(get_element_using_selenium(driver, 'CLASS_NAME', 'average-ratings'))\n    d['reviews'].append(get_element_using_selenium(driver, 'CLASS_NAME', 'image-review-heading'))\n    \n    # Quit the WebDriver to release resources\n    driver.quit()\n    \n    # Return a value (1 in this case, you can customize it as needed)\n    return 1\n","metadata":{"execution":{"iopub.status.busy":"2023-09-23T12:09:37.503482Z","iopub.execute_input":"2023-09-23T12:09:37.504071Z","iopub.status.idle":"2023-09-23T12:09:37.517003Z","shell.execute_reply.started":"2023-09-23T12:09:37.504032Z","shell.execute_reply":"2023-09-23T12:09:37.515130Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# To check user agent being used\n# Get user Agent with execute_script\ndriver_ua = driver.execute_script(\"return navigator.userAgent\")\nprint(\"User agent:\")\nprint(driver_ua)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Function to extract an element using specified attributes from a webpage using Selenium\ndef get_element_using_selenium(driver, attribute, attributeName):\n    try:\n        # Check if the attribute is 'ID'\n        if attribute == 'ID':\n            # Find the element by its ID and extract its text content\n            value = driver.find_element(By.ID, attributeName).text\n        else:\n            # Find the element by its CLASS_NAME and extract its text content\n            value = driver.find_element(By.CLASS_NAME, attributeName).text\n    except Exception as error:\n        # If an error occurs (element not found, for example), set the value to an empty string\n        value = \"\"\n        # Optionally, you can uncomment the following line to print an error message\n        # print('Error occurred while extracting', attributeName, 'Error:', error)\n    \n    # Return the extracted value\n    return value","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"We loop through the product links, scraping details such as title, current price, old price, discount, rating, and reviews for each one. This is where the magic happens. Selenium interacts with the website, and we harvest the data.","metadata":{}},{"cell_type":"code","source":"# Loop for extracting product details from each link\nfor link in links_list:\n    # Construct the full URL by appending the link to the base URL\n    url = \"https://www.tirabeauty.com\" + link\n    \n    # Call the function to scrape product information and store it in the 'd' dictionary\n    get_price_using_selenium(url)\n","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"We store our findings in a Pandas DataFrame, giving us a structured view of the beauty secrets we've unveiled. To keep our data pristine, we clean it up by replacing empty values and dropping rows with missing titles.","metadata":{}},{"cell_type":"code","source":"# Create a DataFrame from the 'd' dictionary\nproducts_df = pd.DataFrame.from_dict(d)\n\n# Replace empty ('') values in the 'title' column with NaN\nproducts_df['title'].replace('', np.nan, inplace=True)\n\n# Drop rows with NaN values in the 'title' column\nproducts_df = products_df.dropna(subset=['title'])\n\n# Save the DataFrame to a CSV file named \"products_csv.csv\" (you can change the filename)\nproducts_df.to_csv(\"products_csv.csv\", header=True, index=False)\n\n# Display the DataFrame if needed\nprint(products_df)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Market Research: Uncovering Insights\n\nThe data we've collected from Tira Beauty's website isn't just a list of cosmetics products. It's a goldmine of insights for market research. Here's how:\n\n**Product Trends**: By analyzing the titles and descriptions, we can identify emerging product trends in the cosmetics industry. Are natural skincare products on the rise? Is there a surge in demand for specific makeup brands?\n\n**Pricing Strategy**: The data includes current and old prices, as well as discounts. This information can help businesses fine-tune their pricing strategies, understand the impact of discounts, and stay competitive.\n\n**Customer Feedback**: Ratings and reviews provide valuable feedback from customers. Brands can use this data to gauge product satisfaction and identify areas for improvement.\n\n**Competitor Analysis**: With data on multiple products, businesses can perform competitive analysis. They can compare their product offerings, prices, and customer feedback against competitors in the market.\n\n**Inventory Management**: Understanding which products are popular and which are not can aid in inventory management. Businesses can stock up on in-demand items and make informed decisions about discontinuing slow-moving products.\n\n**Seasonal Insights:** By analyzing the data over time, businesses can uncover seasonal trends. For example, do certain skincare products sell better in the summer? Are there holiday-themed makeup collections?","metadata":{}}]}